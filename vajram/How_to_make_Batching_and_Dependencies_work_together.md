* [[Simple Input Resolvers]], [[Batching]] [[Code Generation]] and [Dependencies]([[Vajram Dependency]]): When a [[Vajram]] uses all three of these features, we have a problem.  Let us say Vajram `va` has a batched input `bi`, an non batched input `nbi`  and a dependency `d`, and a simple resolver which resolves inputs of `d` using `bi`. The vajram code generator generates two models - BatchedFacets containing the batched inputs and common facets containing the common inputs. But what about the dependency? Which of these two models should contain the value of facet `d`? Theoretically speaking, where d should be added will depend on the sources of the input resolvers resolving its input.  If the inputs of d are being resolved only using non batched inputs like `nbi`, then `d` needs to be in the common inputs class. On the other hand if `d`'s inputs are being resolved using at least one batched input (like `bi`) or any other facets which itself was resolved/computed using a batched input, then `d` needs to be added to the batched facets class. In other words, batched facets class must contain all batched inputs and all other facets which directly or indirectly consume a batched input. This means that the [model gen phase]([[Vajram Model generation phase]]) of the vajram depends on the topological order of the facets defined by how logics consume various facets. If all [resolver]([[Vajram Resolver logic]]) and [compute]([[Vajram Field Compute Logic]]) logics are written as methods annotated with the `@Resolve` annotation,  then the vajram code generator annotation processor can infer the topological order and generate the Batched Facets and Common Facets classes appropriately. But if a resolver logic is written in the [[getSimpleInputResolvers()]] method, using the resolver definition SDK, then the annotation processor has no way to infer the topological order of inter-facet relationships at compile time (because the only way we can retrieve information from the `getSimpleInputResolvers()` method is by creating the vajram object and calling the method which is not possible to do at compile time in an annotation processor). This is the impedance mismatch. This can be solved in one of the following ways:
  * Don't allow Batched Vajrams to have dependencies:  This is the current status of the [framework]([[Krystal Framework]]) (as of version 6). The framework already disallows [[Compute Vajram]]s from using the batching capability (because this is deemed unnecessary and an anti-pattern - the cost of batching is higher than the goodness achieved). This means that this restriction amounts to [[IO Vajram]]s (and [[Delayable Vajram]]s) not being allowed to have dependencies. While this is not a deal-breaker as such, the cons of this approach are
    * It leads to extra code. If an [[IO Vajram]] needs the output of another vajram, the only way to achieve this would be to write a compute vajram which retrieves the outputs of that other vajram and then passes it as input to the IO vajram. 
    * It adds pressure on the cache and CPU. IO Vajrams are, more often than not, cached. Passing the output of another vajram as input the IO vajram means that the complete output of the other vajram will be part of the cache key - meaning its hashcode and equals methods are called every time the IO vajram is invoked (This can be mitigated to some extent by using lazy hashing and equals and caching the hash code in the object - but this is too involved a mitigation).
    * This also creates confusion for the vajrams clients - which vajram to use the compute version of it or the IO (this problem can be mitigated to some extent by introducing vajram visibility/permission feature). 
  * Don't support [[getSimpleInputResolvers()]] feature:  Cons:
    * This would mean the [[Vajram SDK]] will become very verbose - every dependency of every vajram will require its own resolver method.
  * Don't support [[getSimpleInputResolvers()]] for resolvers that consume [[Batched Facet]]s. A [[Batched Facet]] is either a batched [Input]([[Vajram Input]]) or another facet which consumes another batched facet. This can be achieved by validating that none of the resolvers defined in the `getSimpleInputResolvers()` method consume a batched facet. This validation cannot be done by the annotation processor. The only way to do this is to implement a post-compilation validator which loads each [[VajramImpl]] class in the module, creates an instance  of the vajram object, retrieves the simple input resolvers by calling the `getSimpleInputResolvers()` method and then checking that none of the sources of the vajram are batched. The cons of this approach:
    * The developer experience will come a bit confusing. If developers don't know this limitation, they will end up figuring out the problem much later in the development cycle, during validation which runs after the complete vajram has been written. This is not ideal.
  * Don't infer [[Batched Facet]]s: Instead, expect developers to explicitly mark each facet as `@Batch` if it is a [[Batched Facet]]. This is annotation will suffice to make sure the batched facetValues model generation happens correctly. We can still add a post-compile validation to make sure none of the non batched facetValues  consume a batched facet. 
    * Extra code on the part of the developer to write `@Batch` even if it could have been inferred (ideally) - but this is a minor problem, and is a result of verbosity caused by using java to model vajrams. Also, this is still more concise than having to write `@Resolve` methods. 
      * This can be further mitigated by requiring the `@Batch` annotation only on the those dependencies which are being resolved in the `getSimpleInputResolvers()` method and are consuming a batched facet, but this reduced readability. Better to be consistent in this.
